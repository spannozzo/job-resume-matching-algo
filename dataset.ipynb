{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMymPCbs7nHrO2AsBKcNo0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spannozzo/job-resume-matching-algo/blob/master/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmOThVMqJ-e3",
        "outputId": "1acac24a-4530-4c89-ee6f-cb560db693d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'job-resume-matching-algo'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 28 (delta 4), reused 10 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), 17.24 KiB | 1.57 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/ruozhengu/job-resume-matching-algo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "\n",
        "# get soup object\n",
        "def get_soup(text):\n",
        "\treturn BeautifulSoup(text, \"lxml\", from_encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# extract company\n",
        "def extract_company(div):\n",
        "    company = div.find_all(name=\"span\", attrs={\"class\":\"company\"})\n",
        "    if len(company) > 0:\n",
        "        for b in company:\n",
        "            return (b.text.strip())\n",
        "    else:\n",
        "        sec_try = div.find_all(name=\"span\", attrs={\"class\":\"result-link-source\"})\n",
        "        for span in sec_try:\n",
        "            return (span.text.strip())\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# extract job salary\n",
        "def extract_salary(div):\n",
        "    try:\n",
        "        return (div.find('nobr').text)\n",
        "    except:\n",
        "        try:\n",
        "            div_two = div.find(name='span', attrs={'class':'salaryText'})\n",
        "            div_three = div_two.find('div')\n",
        "            salaries.append(div_three.text.strip())\n",
        "        except:\n",
        "            return ('NOT_FOUND')\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# extract job location\n",
        "def extract_location(div):\n",
        "    for span in div.findAll('div', attrs={'class': 'location'}):\n",
        "        return (span.text)\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# extract job title\n",
        "def extract_job_title(div):\n",
        "    for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
        "        return (a['title'])\n",
        "    return('NOT_FOUND')\n",
        "\n",
        "\n",
        "# extract jd summary\n",
        "def extract_summary(div):\n",
        "    spans = div.findAll('div', attrs={'class': 'summary'})\n",
        "    for span in spans:\n",
        "        return (span.text.strip())\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# extract link of job description\n",
        "def extract_link(div):\n",
        "    for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
        "        return (a['href'])\n",
        "    return('NOT_FOUND')\n",
        "\n",
        "\n",
        "# extract date of job when it was posted\n",
        "def extract_date(div):\n",
        "    try:\n",
        "        spans = div.findAll('span', attrs={'class': 'date'})\n",
        "        for span in spans:\n",
        "            return (span.text.strip())\n",
        "    except:\n",
        "        return 'NOT_FOUND'\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# extract full job description from link\n",
        "def extract_fulltext(url):\n",
        "    try:\n",
        "        page = requests.get('http://www.indeed.com' + url)\n",
        "        soup = BeautifulSoup(page.text, \"lxml\", from_encoding=\"utf-8\")\n",
        "        spans = soup.findAll('div', attrs={'class': 'jobsearch-jobDescriptionText'})\n",
        "        for span in spans:\n",
        "            return (span.text.strip())\n",
        "    except:\n",
        "        return 'NOT_FOUND'\n",
        "    return 'NOT_FOUND'\n",
        "\n",
        "\n",
        "# write logs to file\n",
        "def write_logs(text):\n",
        "    # print(text + '\\n')\n",
        "    f = open('log.txt','a')\n",
        "    f.write(text + '\\n')\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "OM3TqJB5M2Vy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "# limit per sity\n",
        "max_results_per_city = 990\n",
        "\n",
        "# db of city\n",
        "#city_set = ['Toronto', 'Markham', 'North+York', 'Scarbough', 'Missisauga', 'Richmond+Hill']\n",
        "city_set = ['Toronto', 'Vancouver', 'alberta', 'waterloo', 'london']\n",
        "# job roles\n",
        "job_set = ['all']\n",
        "\n",
        "\n",
        "df = pd.DataFrame(columns = ['unique_id', 'job_title', 'company_name', 'location', 'full_text'])\n",
        "\n",
        "\n",
        "# loop on all cities\n",
        "for city in city_set:\n",
        "\n",
        "    print(\"Start searching city: \" + city)\n",
        "    cnt = 0\n",
        "\n",
        "    # for results\n",
        "    for start in range(0, max_results_per_city, 10):\n",
        "\n",
        "        # get dom\n",
        "        page = requests.get('http://ca.indeed.com/jobs?q=&l=' + str(city) + '&sort=date&start=' + str(start))\n",
        "\n",
        "        #ensuring at least 1 second between page grabs\n",
        "        time.sleep(1)\n",
        "\n",
        "        #fetch data\n",
        "        soup = get_soup(page.text)\n",
        "        divs = soup.find_all(name=\"div\", attrs={\"class\":\"row\"})\n",
        "\n",
        "        # if results exist\n",
        "        if(len(divs) == 0):\n",
        "            break\n",
        "\n",
        "        # for all jobs on a page\n",
        "        for div in divs:\n",
        "\n",
        "            cnt += 1\n",
        "\n",
        "            #specifying row num for index of job posting in dataframe\n",
        "            num = (len(df) + 1)\n",
        "\n",
        "            #job data after parsing\n",
        "            job_post = []\n",
        "\n",
        "            #append unique id\n",
        "            job_post.append(div['id'])\n",
        "\n",
        "            #grabbing job title\n",
        "            job_post.append(extract_job_title(div))\n",
        "\n",
        "            #grabbing company\n",
        "            job_post.append(extract_company(div))\n",
        "\n",
        "            #grabbing location name\n",
        "            job_post.append(extract_location(div))\n",
        "\n",
        "            #grabbing link\n",
        "            link = extract_link(div)\n",
        "\n",
        "            #grabbing full_text\n",
        "            job_post.append(extract_fulltext(link))\n",
        "\n",
        "            #appending list of job post info to dataframe at index num\n",
        "            df.loc[num] = job_post\n",
        "\n",
        "    print(city + \" is completed with count as \" + str(cnt))\n",
        "\n",
        "#saving df as a local csv file\n",
        "df.to_csv('jobs.csv', encoding='utf-8')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4u6hZaqKNSX",
        "outputId": "36b2d5ac-5e9f-4d4a-9de5-17825291a45d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start searching city: Toronto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bs4/__init__.py:226: UserWarning: You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\n",
            "  warnings.warn(\"You provided Unicode markup but also provided a value for from_encoding. Your from_encoding will be ignored.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toronto is completed with count as 0\n",
            "Start searching city: Vancouver\n",
            "Vancouver is completed with count as 0\n",
            "Start searching city: alberta\n",
            "alberta is completed with count as 0\n",
            "Start searching city: waterloo\n",
            "waterloo is completed with count as 0\n",
            "Start searching city: london\n",
            "london is completed with count as 0\n"
          ]
        }
      ]
    }
  ]
}